{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12032129,"sourceType":"datasetVersion","datasetId":7529513}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:06.602314Z","iopub.execute_input":"2025-06-02T07:37:06.602971Z","iopub.status.idle":"2025-06-02T07:37:07.641079Z","shell.execute_reply.started":"2025-06-02T07:37:06.602943Z","shell.execute_reply":"2025-06-02T07:37:07.640342Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/dpl-2025/Text_Image.csv')\ndf_test = pd.read_csv('/kaggle/input/dpl-2025/Test_Text_Image.csv')\n\nbase_image_dir_train = \"/kaggle/input/dpl-2025/devset_images\" \nbase_image_dir_test = \"/kaggle/input/dpl-2025/testset_images\" \n\ndf_train['image_path'] = df_train['image_path'].apply(lambda x: f\"{base_image_dir_train}/{x}\")\ndf_test['image_path'] = df_test['image_path'].apply(lambda x: f\"{base_image_dir_test}/{x}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:07.642263Z","iopub.execute_input":"2025-06-02T07:37:07.642650Z","iopub.status.idle":"2025-06-02T07:37:07.783985Z","shell.execute_reply.started":"2025-06-02T07:37:07.642630Z","shell.execute_reply":"2025-06-02T07:37:07.783140Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_train = df_train.fillna(\"missing\") \ndef fill_missing(text):\n    if isinstance(text, str) and text.strip() == \"\":\n        return \"missing\"\n    return text\ndf_train = df_train.applymap(fill_missing)\ndf_train.head()\n\ndf_test = df_test.fillna(\"missing\")\ndf_test = df_test.applymap(fill_missing)\ndf_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:07.784747Z","iopub.execute_input":"2025-06-02T07:37:07.784995Z","iopub.status.idle":"2025-06-02T07:37:07.826742Z","shell.execute_reply.started":"2025-06-02T07:37:07.784973Z","shell.execute_reply":"2025-06-02T07:37:07.825777Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3514959801.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_train = df_train.applymap(fill_missing)\n/tmp/ipykernel_35/3514959801.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_test = df_test.applymap(fill_missing)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     image_id                                              title  \\\n0  3483809003  Flooded Parking Lot At Emily Fowler Library, A...   \n1  3712805295             L'arc de Barà / The roman arch of Barà   \n2   379845620  Highest point over the sea level that is reach...   \n3  7343264988                              Lagos after the rains   \n4  3843337492                                 flooded Corley Ave   \n\n                                         description  \\\n0  Denton Creek overflows its banks and floods Oa...   \n1  Sembla que fou dedicat a August entorn l'any 1...   \n2                                            missing   \n3  After heavy rain, Lagos (Nigeria) was still fl...   \n4  also a local black out due to the tree branch ...   \n\n                                           user_tags  \\\n0                    project, slis 5715, spring 2009   \n1  arc, arc_de_berà, arch, archaeology, arco, arq...   \n2                                            missing   \n3                             africa, lagos, nigeria   \n4                              flood, storm, toronto   \n\n                                          image_path  \n0  /kaggle/input/dpl-2025/testset_images/testset_...  \n1  /kaggle/input/dpl-2025/testset_images/testset_...  \n2  /kaggle/input/dpl-2025/testset_images/testset_...  \n3  /kaggle/input/dpl-2025/testset_images/testset_...  \n4  /kaggle/input/dpl-2025/testset_images/testset_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>user_tags</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3483809003</td>\n      <td>Flooded Parking Lot At Emily Fowler Library, A...</td>\n      <td>Denton Creek overflows its banks and floods Oa...</td>\n      <td>project, slis 5715, spring 2009</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3712805295</td>\n      <td>L'arc de Barà / The roman arch of Barà</td>\n      <td>Sembla que fou dedicat a August entorn l'any 1...</td>\n      <td>arc, arc_de_berà, arch, archaeology, arco, arq...</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>379845620</td>\n      <td>Highest point over the sea level that is reach...</td>\n      <td>missing</td>\n      <td>missing</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7343264988</td>\n      <td>Lagos after the rains</td>\n      <td>After heavy rain, Lagos (Nigeria) was still fl...</td>\n      <td>africa, lagos, nigeria</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3843337492</td>\n      <td>flooded Corley Ave</td>\n      <td>also a local black out due to the tree branch ...</td>\n      <td>flood, storm, toronto</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"texts = (\n    df_train['title'].fillna('') + ' ' +\n    df_train['description'].fillna('') + ' ' +\n    df_train['user_tags'].fillna('')\n).tolist()\nimage_paths_train = df_train['image_path'].tolist()\nlabels = df_train['label'].tolist()  # Assuming labels are in a column named 'label'\n\ntest_ids = df_test['image_id'].tolist()\ntest_texts = (\n    df_test['title'].fillna('') + ' ' +\n    df_test['description'].fillna('') + ' ' +\n    df_test['user_tags'].fillna('')\n).tolist()\nimage_paths_test = df_test['image_path'].tolist()\n\ntrain_texts, val_texts, train_labels, val_labels, train_image_paths, val_image_paths = train_test_split(texts, labels, image_paths_train, \n                                                                                      test_size=0.05, stratify=labels, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:07.828575Z","iopub.execute_input":"2025-06-02T07:37:07.828821Z","iopub.status.idle":"2025-06-02T07:37:07.854248Z","shell.execute_reply.started":"2025-06-02T07:37:07.828804Z","shell.execute_reply":"2025-06-02T07:37:07.853606Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Bert + ViT","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Save model when validation loss decreases.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:07.855126Z","iopub.execute_input":"2025-06-02T07:37:07.855634Z","iopub.status.idle":"2025-06-02T07:37:07.869829Z","shell.execute_reply.started":"2025-06-02T07:37:07.855608Z","shell.execute_reply":"2025-06-02T07:37:07.869117Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\nimport torch\n\nfrom PIL import Image, ImageFile\nimport os\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, texts, labels=None, image_paths=None, tokenizer=None, image_transform=None, max_length=128):\n        self.tokenizer = tokenizer\n        self.image_transform = image_transform\n        self.max_length = max_length\n        self.texts = texts\n        self.labels = labels\n        self.image_paths = image_paths\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        image_path = self.image_paths[idx]\n\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.image_transform(image)\n\n        encoding = self.tokenizer(text, max_length=self.max_length,\n                                  padding='max_length', truncation=True, return_tensors='pt')\n        input_ids = encoding['input_ids'].squeeze(0)  \n        attention_mask = encoding['attention_mask'].squeeze(0)\n\n        item = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'pixel_values': image\n        }\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx])\n            item['labels'] = label\n\n        return item\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nimage_transform =transforms.Compose([\n    transforms.Resize((224,224)), \n    transforms.RandomHorizontalFlip(), #Data argumentation\n    transforms.ColorJitter(), #Data argumentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:07.870536Z","iopub.execute_input":"2025-06-02T07:37:07.870765Z","iopub.status.idle":"2025-06-02T07:37:27.103740Z","shell.execute_reply.started":"2025-06-02T07:37:07.870742Z","shell.execute_reply":"2025-06-02T07:37:27.103002Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd8bc867bb04b48b33b31425dc3b426"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b29ff666f3fa4000a833da1be10b0930"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62759d0e3562433a93d8279513e9067d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72a4f1ce76f84f32a995074408505381"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"train_dataset = MultiModalDataset(train_texts, train_labels, train_image_paths, tokenizer, image_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\nval_dataset = MultiModalDataset(val_texts, val_labels, val_image_paths, tokenizer, image_transform)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n\ntest_dataset = MultiModalDataset(test_texts, None, image_paths_test, tokenizer, image_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:27.104427Z","iopub.execute_input":"2025-06-02T07:37:27.104828Z","iopub.status.idle":"2025-06-02T07:37:27.110144Z","shell.execute_reply.started":"2025-06-02T07:37:27.104801Z","shell.execute_reply":"2025-06-02T07:37:27.109365Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel, ViTModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\nclass CombinedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224')\n\n        bert_hidden = self.bert.config.hidden_size\n        vit_hidden = self.vit.config.hidden_size\n\n        self.classifier = nn.Sequential(\n            nn.Linear(bert_hidden + vit_hidden, 512), #Tổng chiều đầu ra của torch.cat((bert_cls, vit_cls), dim=1) = bert_dim + vit_dim → Phải khớp với input size của nn.Linear\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 2)\n        )\n    \n    def forward(self, input_ids, attention_mask, pixel_values):\n        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_cls = bert_out.last_hidden_state[:, 0, :]  # CLS token\n\n        vit_out = self.vit(pixel_values=pixel_values)\n        vit_cls = vit_out.last_hidden_state[:, 0, :]\n\n        combined = torch.cat((bert_cls, vit_cls), dim=1)\n        logits = self.classifier(combined)\n        #return logits.squeeze(1) # batch size = 1: return logits.view(-1)\n        return SequenceClassifierOutput(logits=logits)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:27.111085Z","iopub.execute_input":"2025-06-02T07:37:27.111373Z","iopub.status.idle":"2025-06-02T07:37:46.893430Z","shell.execute_reply.started":"2025-06-02T07:37:27.111353Z","shell.execute_reply":"2025-06-02T07:37:46.892850Z"}},"outputs":[{"name":"stderr","text":"2025-06-02 07:37:30.080539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748849850.475533      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748849850.593684      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def validation(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_samples = 0, 0, 0\n\n    with torch.no_grad():\n        loop = tqdm(val_loader, leave=True)\n        for batch in loop:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask, pixel_values)\n            logits = outputs.logits\n            loss = criterion(logits, labels)\n\n            #preds = torch.sigmoid(outputs) >= 0.5       \n            preds = torch.argmax(logits, dim=1)\n                        \n            total_loss += loss.item() * labels.size(0)\n            total_correct += (preds == labels.long()).sum().item()\n            total_samples += labels.size(0)\n \n    avg_loss = total_loss / total_samples\n    accuracy = total_correct / total_samples\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:46.894149Z","iopub.execute_input":"2025-06-02T07:37:46.894599Z","iopub.status.idle":"2025-06-02T07:37:46.901431Z","shell.execute_reply.started":"2025-06-02T07:37:46.894579Z","shell.execute_reply":"2025-06-02T07:37:46.900628Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CombinedModel().to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)\ncriterion = nn.CrossEntropyLoss()\n\nnum_epochs = 10\nmodel.train()\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in range(num_epochs):\n    loop = tqdm(train_loader, leave=True)\n    total_loss, total_accuracy, total_samples = 0, 0, 0\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, pixel_values)\n        logits = outputs.logits  # Remove it if not use huggingface\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n\n        # probs = torch.sigmoid(outputs)\n        # preds = (probs > 0.5).long()\n        preds = torch.argmax(logits, dim=1)\n        \n        total_loss += loss.item() * labels.size(0)\n        total_accuracy += (preds == labels.long()).sum().item()\n        total_samples += labels.size(0)\n        loop.set_description(f'Epoch {epoch+1}')\n        loop.set_postfix(loss=loss.item())  \n        \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/total_samples:.4f}, Train Accuracy: {total_accuracy/total_samples:.4f}\")\n\n    val_loss, val_acc = validation(model, val_loader, criterion, device)\n    print(f\"Epoch {epoch+1}: Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n\n    early_stopper(val_loss, model)\n    if early_stopper.early_stop:\n        print(\"Early stopping\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T07:37:46.904701Z","iopub.execute_input":"2025-06-02T07:37:46.905089Z","execution_failed":"2025-06-02T08:28:39.638Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca200f58214c4933ba2c4bb542dba737"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5340bdf8014c46d98d8038dd9bc48f14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d75f9e0122b41098466d43159a7a835"}},"metadata":{}},{"name":"stderr","text":"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1: 100%|██████████| 157/157 [04:48<00:00,  1.84s/it, loss=0.239]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Train Loss: 0.5209, Train Accuracy: 0.7438\n","output_type":"stream"},{"name":"stderr","text":"Epoch [1/10]: 100%|██████████| 9/9 [00:06<00:00,  1.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Val Loss=0.3050, Val Acc=0.8977\nValidation loss decreased (inf → 0.304969). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 157/157 [04:53<00:00,  1.87s/it, loss=0.237] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Train Loss: 0.1801, Train Accuracy: 0.9310\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|██████████| 9/9 [00:05<00:00,  1.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Val Loss=0.1335, Val Acc=0.9470\nValidation loss decreased (0.304969 → 0.133506). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 157/157 [04:53<00:00,  1.87s/it, loss=0.0511]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Train Loss: 0.0888, Train Accuracy: 0.9715\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|██████████| 9/9 [00:06<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Val Loss=0.1258, Val Acc=0.9470\nValidation loss decreased (0.133506 → 0.125829). Saving model ...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 157/157 [04:53<00:00,  1.87s/it, loss=0.00388]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Train Loss: 0.0407, Train Accuracy: 0.9892\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|██████████| 9/9 [00:05<00:00,  1.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Val Loss=0.1509, Val Acc=0.9356\nEarlyStopping counter: 1 out of 3\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 157/157 [04:53<00:00,  1.87s/it, loss=0.024]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Train Loss: 0.0129, Train Accuracy: 0.9986\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|██████████| 9/9 [00:06<00:00,  1.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Val Loss=0.1596, Val Acc=0.9432\nEarlyStopping counter: 2 out of 3\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6:  80%|████████  | 126/157 [03:55<00:57,  1.86s/it, loss=0.00145] ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"model.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n        pixel_values = batch['pixel_values'].to('cuda' if torch.cuda.is_available() else 'cpu')\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n        \n        # probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n        # preds = (probs > 0.5).astype(int)\n        logits = outputs.logits # if use model huggingface to return result need .logit to get output\n        preds = torch.argmax(logits, dim=-1)\n        all_preds.extend(preds.cpu().numpy())\n\ndf_test['label'] = all_preds\ndf_test[['image_id', 'label']].to_csv(\"predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-02T08:28:39.638Z"}},"outputs":[],"execution_count":null}]}