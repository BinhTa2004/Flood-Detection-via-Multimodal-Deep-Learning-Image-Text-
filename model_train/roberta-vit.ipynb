{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12032129,"sourceType":"datasetVersion","datasetId":7529513}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data processing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport os\n\nfrom PIL import Image, ImageFile\nimport os\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\ndf_train = pd.read_csv(r'/kaggle/input/dpl-2025/devset_images_metadata/devset_images_metadata/devset_images_metadata_cleaned.csv')\nimage_folder_train = r'/kaggle/input/dpl-2025/devset_images/devset_images'\ndf_train_labels = pd.read_csv('/kaggle/input/dpl-2025/devset_images_gt.csv')\n\nextensions = ['.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff', '.webp']\n\ndef find_image_path(image_id):\n    for ext in extensions:\n        path = os.path.join(image_folder_train, f\"{image_id}{ext}\")\n        if os.path.exists(path):\n            return path\n    # Nếu không tìm thấy file nào, trả về None hoặc mặc định đuôi .jpg\n    return os.path.join(image_folder_train, f\"{image_id}.jpg\")\n\ndf_train['image_path'] = df_train['image_id'].apply(find_image_path)\ndf_merged_train = pd.merge(df_train, df_train_labels, left_on=\"image_id\", right_on=\"id\", how=\"inner\")\n\n\ndf_test = pd.read_csv(r'/kaggle/input/dpl-2025/test.csv')\nimage_folder_test = r'/kaggle/input/dpl-2025/testset_images/testset_images'\n\ndef find_image_path_test(image_id):\n    for ext in extensions:\n        path = os.path.join(image_folder_test, f\"{image_id}{ext}\")\n        if os.path.exists(path):\n            return path\n    return os.path.join(image_folder_test, f\"{image_id}.jpg\")\n\ndf_test['image_path'] = df_test['image_id'].apply(find_image_path_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:20.909074Z","iopub.execute_input":"2025-06-27T08:29:20.909588Z","iopub.status.idle":"2025-06-27T08:29:44.335845Z","shell.execute_reply.started":"2025-06-27T08:29:20.909564Z","shell.execute_reply":"2025-06-27T08:29:44.335066Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df_train[\"image_path\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.337180Z","iopub.execute_input":"2025-06-27T08:29:44.337463Z","iopub.status.idle":"2025-06-27T08:29:44.344505Z","shell.execute_reply.started":"2025-06-27T08:29:44.337438Z","shell.execute_reply":"2025-06-27T08:29:44.343737Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0       /kaggle/input/dpl-2025/devset_images/devset_im...\n1       /kaggle/input/dpl-2025/devset_images/devset_im...\n2       /kaggle/input/dpl-2025/devset_images/devset_im...\n3       /kaggle/input/dpl-2025/devset_images/devset_im...\n4       /kaggle/input/dpl-2025/devset_images/devset_im...\n                              ...                        \n5275    /kaggle/input/dpl-2025/devset_images/devset_im...\n5276    /kaggle/input/dpl-2025/devset_images/devset_im...\n5277    /kaggle/input/dpl-2025/devset_images/devset_im...\n5278    /kaggle/input/dpl-2025/devset_images/devset_im...\n5279    /kaggle/input/dpl-2025/devset_images/devset_im...\nName: image_path, Length: 5280, dtype: object"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"columns = [\"image_id\", \"title\",\"description\", \"user_tags\", \"label\",\"image_path\"]\n#columns = [\"image_id\",\"title\",\"description\", \"user_tags\", \"latitude\", \"longitude\",\"label\"]\ndf_merged_train = df_merged_train[columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.345328Z","iopub.execute_input":"2025-06-27T08:29:44.345597Z","iopub.status.idle":"2025-06-27T08:29:44.358345Z","shell.execute_reply.started":"2025-06-27T08:29:44.345572Z","shell.execute_reply":"2025-06-27T08:29:44.357618Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"df_merged_train = df_merged_train.fillna(\"missing\") \ndef fill_missing(text):\n    if isinstance(text, str) and text.strip() == \"\":\n        return \"missing\"\n    return text\ndf_merged_train = df_merged_train.applymap(fill_missing)\ndf_merged_train.head()\n\ndf_test = df_test.fillna(\"missing\")\ndf_test = df_test.applymap(fill_missing)\ndf_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.359103Z","iopub.execute_input":"2025-06-27T08:29:44.359369Z","iopub.status.idle":"2025-06-27T08:29:44.399693Z","shell.execute_reply.started":"2025-06-27T08:29:44.359345Z","shell.execute_reply":"2025-06-27T08:29:44.398961Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/2548289923.py:6: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_merged_train = df_merged_train.applymap(fill_missing)\n/tmp/ipykernel_35/2548289923.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_test = df_test.applymap(fill_missing)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     image_id                                              title  \\\n0  3483809003  Flooded Parking Lot At Emily Fowler Library, A...   \n1  3712805295             L'arc de Barà / The roman arch of Barà   \n2   379845620  Highest point over the sea level that is reach...   \n3  7343264988                              Lagos after the rains   \n4  3843337492                                 flooded Corley Ave   \n\n                                         description  \\\n0  Denton Creek overflows its banks and floods Oa...   \n1  Sembla que fou dedicat a August entorn l'any 1...   \n2                                            missing   \n3  After heavy rain, Lagos (Nigeria) was still fl...   \n4  also a local black out due to the tree branch ...   \n\n                                           user_tags  \\\n0                    project, slis 5715, spring 2009   \n1  arc, arc_de_berà, arch, archaeology, arco, arq...   \n2                                            missing   \n3                             africa, lagos, nigeria   \n4                              flood, storm, toronto   \n\n                                          image_path  \n0  /kaggle/input/dpl-2025/testset_images/testset_...  \n1  /kaggle/input/dpl-2025/testset_images/testset_...  \n2  /kaggle/input/dpl-2025/testset_images/testset_...  \n3  /kaggle/input/dpl-2025/testset_images/testset_...  \n4  /kaggle/input/dpl-2025/testset_images/testset_...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>title</th>\n      <th>description</th>\n      <th>user_tags</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3483809003</td>\n      <td>Flooded Parking Lot At Emily Fowler Library, A...</td>\n      <td>Denton Creek overflows its banks and floods Oa...</td>\n      <td>project, slis 5715, spring 2009</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3712805295</td>\n      <td>L'arc de Barà / The roman arch of Barà</td>\n      <td>Sembla que fou dedicat a August entorn l'any 1...</td>\n      <td>arc, arc_de_berà, arch, archaeology, arco, arq...</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>379845620</td>\n      <td>Highest point over the sea level that is reach...</td>\n      <td>missing</td>\n      <td>missing</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7343264988</td>\n      <td>Lagos after the rains</td>\n      <td>After heavy rain, Lagos (Nigeria) was still fl...</td>\n      <td>africa, lagos, nigeria</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3843337492</td>\n      <td>flooded Corley Ave</td>\n      <td>also a local black out due to the tree branch ...</td>\n      <td>flood, storm, toronto</td>\n      <td>/kaggle/input/dpl-2025/testset_images/testset_...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"texts = (\n    df_merged_train['title'].fillna('') + ' ' +\n    df_merged_train['description'].fillna('') + ' ' +\n    df_merged_train['user_tags'].fillna('')\n).tolist()\n\nimage_paths_train = df_merged_train['image_path'].tolist()\nlabels = df_merged_train['label'].tolist()\n\ntest_ids = df_test['image_id'].tolist()\ntest_texts = (\n    df_test['title'].fillna('') + ' ' +\n    df_test['description'].fillna('') + ' ' +\n    df_test['user_tags'].fillna('')\n).tolist()\nimage_paths_test = df_test['image_path'].tolist()\n\ntrain_texts, val_texts, train_labels, val_labels, train_image_paths, val_image_paths = train_test_split(texts, labels, image_paths_train, \n                                                                                      test_size=0.1, stratify=labels, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.401508Z","iopub.execute_input":"2025-06-27T08:29:44.401764Z","iopub.status.idle":"2025-06-27T08:29:44.422114Z","shell.execute_reply.started":"2025-06-27T08:29:44.401748Z","shell.execute_reply":"2025-06-27T08:29:44.421500Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Early Stopping","metadata":{}},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=5, verbose=False, delta=0, path='checkpoint.pt'):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Save model when validation loss decreases.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} → {val_loss:.6f}). Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.422831Z","iopub.execute_input":"2025-06-27T08:29:44.423083Z","iopub.status.idle":"2025-06-27T08:29:44.430205Z","shell.execute_reply.started":"2025-06-27T08:29:44.423065Z","shell.execute_reply":"2025-06-27T08:29:44.429431Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Bert + ViT","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom tqdm import tqdm\nimport torch\n\nclass MultiModalDataset(Dataset):\n    def __init__(self, texts, labels=None, image_paths=None, tokenizer=None, image_transform = None, max_length = 40):\n        self.tokenizer = tokenizer\n        self.image_transform = image_transform\n        self.max_length = max_length\n        self.texts = texts\n        self.labels = labels\n        self.image_paths = image_paths\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self,idx):\n        text = self.texts[idx]\n        image_path = self.image_paths[idx]\n\n        image = Image.open(image_path).convert(\"RGB\")\n        image = self.image_transform(image)\n        \n        encoding = self.tokenizer(text, max_length=self.max_length,\n                                  padding='max_length', truncation=True, return_tensors='pt')\n        input_ids = encoding['input_ids'].squeeze(0)  \n        attention_mask = encoding['attention_mask'].squeeze(0)\n\n        item = {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'pixel_values': image\n        }\n\n        if self.labels is not None:\n            label = torch.tensor(self.labels[idx])\n            item['labels'] = label\n\n        return item\n        \ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # roberta-base\n\nimage_transform =transforms.Compose([\n    transforms.Resize((224,224)), \n    transforms.RandomHorizontalFlip(), #Data argumentation\n    transforms.ColorJitter(), #Data argumentation\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:44.431015Z","iopub.execute_input":"2025-06-27T08:29:44.431218Z","iopub.status.idle":"2025-06-27T08:29:56.256646Z","shell.execute_reply.started":"2025-06-27T08:29:44.431194Z","shell.execute_reply":"2025-06-27T08:29:56.255875Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5ed7b6842ec4450862bbc299e4f89da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fc37067684641a2ae9e557e8dc51cf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87381f6a3694b15af1a79e2792cd9eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20237692503a42169c2c3228f4dc9eb9"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_dataset = MultiModalDataset(train_texts, train_labels, train_image_paths, tokenizer, image_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n\nval_dataset = MultiModalDataset(val_texts, val_labels, val_image_paths, tokenizer, image_transform)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=True, num_workers=4)\n\ntest_dataset = MultiModalDataset(test_texts, None, image_paths_test, tokenizer, image_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:56.257511Z","iopub.execute_input":"2025-06-27T08:29:56.258012Z","iopub.status.idle":"2025-06-27T08:29:56.264270Z","shell.execute_reply.started":"2025-06-27T08:29:56.257984Z","shell.execute_reply":"2025-06-27T08:29:56.263403Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport timm\n\nclass CombinedModel_need_fix(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('roberta-base') #bert-base-uncased\n        # Load Swin Transformer từ timm (pretrained ImageNet)\n        self.swin = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n        self.swin_head_dim = self.swin.head.in_features\n        self.swin.head = nn.Identity()  # bỏ classifier head đi, chỉ lấy feature\n\n        bert_hidden = self.bert.config.hidden_size  # thường là 768\n        swin_hidden = self.swin_head_dim  # thường là 1024 với swin_base\n\n        self.classifier = nn.Sequential(\n            nn.Linear(bert_hidden + swin_hidden, 512),\n            # nn.BatchNorm1d(1024),\n            # nn.GELU(),\n            # nn.Dropout(0.4),\n            # nn.Linear(1024,512)\n            nn.BatchNorm1d(512),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(512,256),\n            nn.BatchNorm1d(256),\n            nn.GELU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 1)\n        )\n\n        self.fusion = nn.TransformerEncoder(    \n            nn.TransformerEncoderLayer(\n                d_model=bert_hidden + swin_hidden,\n                nhead=8,\n                dim_feedforward=512,\n                dropout=0.1,\n                activation='gelu'\n            ),\n            num_layers=2 \n        )  \n        \n    def forward(self, input_ids, attention_mask, pixel_values):\n        # bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        # bert_cls = bert_out.last_hidden_state[:, 0, :]  # CLS token\n        \n        # # SWIN part\n        # swin_out = self.swin(pixel_values)  # (batch_size, C, H, W) nếu timm trả về 4D # output shape: (batch_size, swin_hidden)\n        # if swin_out.dim() == 4:\n        #     swin_out = swin_out.mean(dim=[2,3])  # global average pooling → (batch_size, C)\n\n        # combined = torch.cat((bert_cls, swin_out), dim=1)  # concat two feature vectors: (batch_size, bert_hidden + swin_hidden)\n        # # Fusion layer\n        # combined_fused = self.fusion(combined.unsqueeze(1)).squeeze(1)\n        \n        # logits = self.classifier(combined_fused)\n        # return logits.squeeze(1) # batch size = 1: return logits.view(-1)\n        # #return SequenceClassifierOutput(logits=logits)\n        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_cls = bert_out.last_hidden_state[:, 0, :]\n        print(\"bert_cls.shape\", bert_cls.shape)\n\n        swin_out = self.swin(pixel_values)\n        print(\"raw swin_out.shape\", swin_out.shape)\n\n        if swin_out.dim() == 4:\n            swin_out = swin_out.mean(dim=[2, 3])\n            print(\"pooled swin_out.shape\", swin_out.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:29:56.265260Z","iopub.execute_input":"2025-06-27T08:29:56.265512Z","iopub.status.idle":"2025-06-27T08:30:11.231368Z","shell.execute_reply.started":"2025-06-27T08:29:56.265491Z","shell.execute_reply":"2025-06-27T08:30:11.230779Z"}},"outputs":[{"name":"stderr","text":"2025-06-27 08:29:57.330604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751012997.459391      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751012997.497186      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import ViltProcessor, ViltModel\nclass ViLTClassifier(nn.Module):\n    def __init__(self, hidden_size=768):\n        super(ViLTClassifier, self).__init__()\n        self.vilt = ViltModel.from_pretrained(\"dandelin/vilt-b32-mlm\")\n        self.classifier = nn.Linear(hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask, pixel_values, token_type_ids=None):\n        outputs = self.vilt(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            pixel_values=pixel_values,\n            token_type_ids=token_type_ids\n        )\n        pooled = outputs.pooler_output\n        return self.classifier(pooled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:30:11.232028Z","iopub.execute_input":"2025-06-27T08:30:11.232446Z","iopub.status.idle":"2025-06-27T08:30:11.293265Z","shell.execute_reply.started":"2025-06-27T08:30:11.232429Z","shell.execute_reply":"2025-06-27T08:30:11.292763Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import BertModel\nimport timm\n\nclass CombinedModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Text Encoder: BERT\n        self.bert = BertModel.from_pretrained('roberta-base')  # hoặc 'bert-base-uncased'\n        bert_hidden = self.bert.config.hidden_size  # 768\n\n        # Image Encoder: Swin Transformer\n        self.swin = timm.create_model('swin_base_patch4_window7_224', pretrained=True)\n        self.swin.head = nn.Identity()\n        swin_hidden = self.swin.num_features  # 1024\n\n        self.bert_hidden = bert_hidden\n        self.swin_hidden = swin_hidden\n\n        # Fusion Layer (Transformer Encoder)\n        self.fusion = nn.TransformerEncoder(    \n            nn.TransformerEncoderLayer(\n                d_model=bert_hidden + swin_hidden,\n                nhead=8,\n                dim_feedforward=512,\n                dropout=0.1,\n                activation='gelu'\n            ),\n            num_layers=2 \n        )\n\n        # Classifier Head\n        self.classifier = nn.Sequential(\n            nn.Linear(bert_hidden + swin_hidden, 512),\n            nn.BatchNorm1d(512),\n            nn.GELU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.GELU(),\n            nn.Dropout(0.4),\n            nn.Linear(256, 1)\n        )\n    \n    def forward(self, input_ids, attention_mask, pixel_values):\n        # BERT Encoding\n        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        bert_cls = bert_out.last_hidden_state[:, 0, :]  # (batch_size, 768)\n\n        # SWIN Encoding\n        swin_out = self.swin(pixel_values)  # shape (batch_size, H, W, C)\n        \n        if swin_out.dim() == 4:\n            swin_out = swin_out.permute(0, 3, 1, 2)  # (batch_size, C, H, W)\n            swin_out = swin_out.mean(dim=[2, 3])      # Global Average Pooling → (batch_size, 1024)\n\n        # Concatenate features\n        combined = torch.cat((bert_cls, swin_out), dim=1)  # (batch_size, 1792)\n\n        # Fusion Transformer Encoder\n        combined_fused = self.fusion(combined.unsqueeze(1)).squeeze(1)  # (batch_size, 1792)\n\n        # Classification head\n        logits = self.classifier(combined_fused)\n\n        return logits.squeeze(1)  # (batch_size,)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:30:11.293921Z","iopub.execute_input":"2025-06-27T08:30:11.294159Z","iopub.status.idle":"2025-06-27T08:30:11.463759Z","shell.execute_reply.started":"2025-06-27T08:30:11.294143Z","shell.execute_reply":"2025-06-27T08:30:11.463022Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# class ResidualBlock(nn.Module):\n#     def __init__(self, in_features):\n#         super().__init__()\n#         self.block = nn.Sequential(\n#             nn.Linear(in_features, in_features),\n#             nn.BatchNorm1d(in_features),\n#             nn.ReLU(),\n#             nn.Dropout(0.1)\n#         )\n\n#     def forward(self, x):\n#         return x + self.block(x)\n\n# # Áp dụng trong classifier\n# self.classifier = nn.Sequential(\n#     nn.Linear(bert_hidden + vit_hidden, 512),\n#     ResidualBlock(512),\n#     nn.Linear(512, 2)\n# )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:30:11.464649Z","iopub.execute_input":"2025-06-27T08:30:11.464956Z","iopub.status.idle":"2025-06-27T08:30:11.477231Z","shell.execute_reply.started":"2025-06-27T08:30:11.464931Z","shell.execute_reply":"2025-06-27T08:30:11.476433Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def validation(model, dataloader, criterion, device):\n    model.eval()\n    total_loss, total_correct, total_samples = 0, 0, 0\n\n    with torch.no_grad():\n        loop = tqdm(val_loader, leave=True)\n        for batch in loop:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device).float()\n\n            outputs = model(input_ids, attention_mask, pixel_values).squeeze(1)\n            # logits = outputs.logits\n            # loss = criterion(logits, labels)\n            loss = criterion(outputs, labels)\n\n            preds = torch.sigmoid(outputs) >= 0.5       \n            #preds = torch.argmax(logits, dim=1)\n                        \n            total_loss += loss.item() * labels.size(0)\n            #total_accuracy += (preds == labels.long()).sum().item()\n            total_correct += (preds == labels).sum().item()\n            total_samples += labels.size(0)\n \n    avg_loss = total_loss / total_samples\n    accuracy = total_correct / total_samples\n    return avg_loss, accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:30:11.478112Z","iopub.execute_input":"2025-06-27T08:30:11.478378Z","iopub.status.idle":"2025-06-27T08:30:11.484865Z","shell.execute_reply.started":"2025-06-27T08:30:11.478362Z","shell.execute_reply":"2025-06-27T08:30:11.484189Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#model = CombinedModel()\nmodel = ViLTClassifier()\nmodel = nn.DataParallel(model).to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-6, weight_decay=0.01) # to reduce overfiting \n# criterion = nn.CrossEntropyLoss()\ncriterion = nn.BCEWithLogitsLoss()\n\nnum_epochs = 10\nmodel.train()\nearly_stopper = EarlyStopping(patience=3, verbose=True)\n\nfor epoch in range(num_epochs):\n    loop = tqdm(train_loader, leave=True)\n    total_loss, total_accuracy, total_samples = 0, 0, 0\n    for batch in loop:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['labels'].to(device).float()\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, pixel_values).squeeze(1) # use vilt thì thêm .squeeze(1) ở output\n        # logits = outputs.logits  # Remove it if not use huggingface\n        # loss = criterion(logits, labels)\n        loss = criterion(outputs, labels) \n        loss.backward()\n        optimizer.step()\n\n        probs = torch.sigmoid(outputs)\n        preds = (probs > 0.5).float()\n        #preds = torch.argmax(logits, dim=1)\n        \n        total_loss += loss.item() * labels.size(0)\n        #total_accuracy += (preds == labels.long()).sum().item() # Cross entropy\n        total_accuracy += (preds == labels).sum().item() \n        total_samples += labels.size(0)\n        \n        loop.set_description(f'Epoch {epoch+1}')\n        loop.set_postfix(loss=loss.item())  \n        \n    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {total_loss/total_samples:.4f}, Train Accuracy: {total_accuracy/total_samples:.4f}\")\n\n    val_loss, val_acc = validation(model, val_loader, criterion, device)\n    print(f\"Epoch {epoch+1}: Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}\")\n\n    early_stopper(val_loss, model)\n    if early_stopper.early_stop:\n        print(\"Early stopping\")\n        break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nall_preds = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to('cuda' if torch.cuda.is_available() else 'cpu')\n        attention_mask = batch['attention_mask'].to('cuda' if torch.cuda.is_available() else 'cpu')\n        pixel_values = batch['pixel_values'].to('cuda' if torch.cuda.is_available() else 'cpu')\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n        \n        probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n        preds = (probs > 0.5).astype(int)\n        all_preds.extend(preds)\n        # logits = outputs.logits # if use model huggingface to return result need .logit to get output\n        # preds = torch.argmax(logits, dim=-1)\n        #all_preds.extend(preds.cpu().numpy())\n\ndf_test['label'] = all_preds\ndf_test[['image_id', 'label']].to_csv(\"test.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:38:23.117746Z","iopub.execute_input":"2025-06-27T08:38:23.118104Z","iopub.status.idle":"2025-06-27T08:38:30.884434Z","shell.execute_reply.started":"2025-06-27T08:38:23.118078Z","shell.execute_reply":"2025-06-27T08:38:30.883694Z"}},"outputs":[],"execution_count":16}]}